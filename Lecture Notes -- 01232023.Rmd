---
output:
  html_document: default
---
```{r setup, include = FALSE}
# This line of code tells the document all the display defaults

knitr::opts_chunk$set(echo = TRUE, cache = TRUE, include = TRUE, fig.align="center", fig.width = 3, fig.height = 3)
```

## Important Definitions

- **Statistics**: the science and art of collecting, analyzing, and drawing conclusions from data.

- **Population of Interest**: Group of individuals we wish to know more information about

- **Sample**: Subset of the population of interest from which we can obtain information

- **Individuals**: the subjects/objects of the population of interest; can be people, but also business firms, common stocks, or any other object we want to study.

- **Variable**: any characteristic of an individual that we can measure and observe.

## Uploading a dataset to R

```{r, include = TRUE}
# This is a very common R package that easily creates tidy data easier for
# analysis and make beautiful graphs!
library(tidyverse)

# This tells R to upload a csv as a data frame
airfares <- read.csv("../Data/airfares.csv")
# <- will define an object in R for us
# read.csv uploads a dataframe to R

# This tells R to define an object as a variable from the data frame
dist <- airfares$Distance
# dataframe$variable
```

\newpage

## Parameters and Statistics

- **Population Parameter**: A numeric value that describes the characteristics of an entire population

- **Sample Statistic**: A numeric value that describes the characteristics of the observed data from a sample

Recall, we use **sample statistics** to make inference about **population parameters**.

Some important sample statistics:

- **Sample Mean**: $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$
- **Sample Variance**: $s_x^2 = \frac{1}{n - 1}\sum_{i=1}^n (x_i - \bar{x})^2$
- **Sample Standard Deviation**: $s_x = \sqrt{s_x^2}$

## Summary Statistics in R
```{r, include = TRUE}
# dist is defined previously as airfares$distance
mean(dist) # sample mean
median(dist) # sample median
var(dist) # sample variance
sd(dist) # sample standard deviation
summary(dist) # five-number summary with mean
```

## Summary Graphs in R
```{r, include = TRUE}
# Create a histogram in R
hist(dist)

# Change the breaks in a histogram in R
hist(dist,breaks=7)

# Create a boxplot in R
boxplot(dist)
```

# Random Variables and Distributions

- **Random Variable** denote a variable whose observed values may be considered outcomes of a stochastic or random experiment.  Random variables are typically denoted by a capital letter $X$, $Y$, etc., while observations are typically denoted by lowercase letters $x$, $y$, etc.

- Note: When a dollar sign is outside of R code in the main text, this is LaTeX math notation

Recall, a data frame contains **observations** from multiple **random variables** from a particular **sample** from the **population of interest**.

## Normal Distribution

If a random variable $X$ is normally distributed, this is denoted as $$X \sim \mathcal{N}(\mu_x,\sigma_x^2)$$ where $\mu_x$ is the mean of $X$ and $\sigma_x$ is the standard deviation of $X$.

### Example

Suppose $X \sim \mathcal{N}(2,4)$.  

### a 
What is $Pr(X > 3.5)$?
```{r, include = TRUE}
# We want to calculate a probability (p)
# From a normal distribution (norm)
1 - pnorm(q=3.5,mean=2,sd=2)
pnorm(q=3.5,mean=2,sd=2,lower.tail=FALSE)
```

### b
What is the $0.35$ quantile/$35^{th}$ percentile of $X$?
```{r, include = TRUE}
# We want to calculate a quantile (q)
# From a normal distribution (norm)
qnorm(p=0.35,mean=2,sd=2)
```

## Central Limit Theorem

Define $\bar{X}$ as the random variable associated with the mean of a sample $\bar{x}$.

If a random variable $X$ is normally distributed with mean $\mu_x$ and standard deviation $\sigma_x$ OR the sample size $n_x$ is sufficiently large ($n_x > 30$), then the **sampling distribution of the sample mean**, $$\bar{X} \approx \mathcal{N}\left(\mu_x,\frac{\sigma_x^2}{n_x}\right)$$ or, in other words, $$\frac{\bar{X} - \mu_x}{\frac{\sigma_x}{\sqrt{n_x}}} \approx \mathcal{N}(0,1).$$  This is an important theorem used in estimation and inference, and will be used throughout the semester.

## Chi-squared $\chi^2$ Distribution

Let $S_x^2$ be the random variable associated with the sample variance $s_x^2$.  The chi-squared distribution can be used to describe the distribution of $S_x^2$, among other types of random variables.  More specifically, $$\frac{(n_x-1)S_x^2}{\sigma_x^2} \sim \chi^2_{df = n_x-1}.$$  The chi-squared distribution applies only to positive random variables and is significantly skewed to the right.

```{r, echo = FALSE}
ggplot(data = data.frame(x = c(0, 20)), aes(x)) +
  stat_function(fun = dchisq, n = 101, args = list(df = 6)) + 
  ylab("") +
  scale_y_continuous(breaks = NULL) + 
  theme_bw() + 
  theme(legend.position = "none")
```

## $t$ Distribution

Often times, the population standard deviation $\sigma_x$ is unknown in the purposes of the sampling distribution.  If this is the case, then we can substitute the sample standard deviation $s_x$ for the population standard deviation, $\sigma_x$.  And, in that case, $$\frac{\bar{X} - \mu_x}{\frac{s_x}{\sqrt{n_x}}} \sim t_{df = n_x-1}.$$

Like the normal distribution, the $t$ distribution is also symmetric and unimodal, but has fatter tails to account for the fact that we are using an estimate $s_x$ instead of $\sigma_x$.

```{r, echo = FALSE}
ggplot(data = data.frame(x = c(-4, 4)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) +
  stat_function(fun = dt, n = 101, args = list(df = 3), aes(colour = "red")) +
  ylab("") +
  scale_y_continuous(breaks = NULL) + 
  theme_bw() + 
  theme(legend.position = "none")
```

### Example

Suppose $X \sim t_{df=10}$.  

### a 
What is $Pr(X < 2.2)$?
```{r, include = TRUE}
# We want to calculate a probability (p)
# From a t-distribution (t)
pt(2.2,df=10)
pt(2.2,10)
```

### b
What is the $0.8$ quantile/$80^{th}$ percentile of $X$?
```{r, include = TRUE}
# We want to calculate a quantile (q)
# from a t-distribution(t)
qt(0.8,df=10)
```

## $F$ Distribution

Suppose now we have a new set of data from a random variable $Y$ with population mean $\mu_y$ and population variance $\sigma_y^2$.  Suppose the observed data has a sample mean $\bar{y}$ and sample variance $s_y^2$.  The $F$ distribution is an appropriate distribution for the ratio of the variances of the two random variables.  More specifically, $$\frac{S_x^2/\sigma_x^2}{S_y^2/\sigma_y^2} \sim F_{df1 = n_x - 1,df2 = n_y - 1}$$ where $df1$ is denoted as the **numerator degrees of freedom** and $df2$ is denoted as the **denominator degrees of freedom**.  The $F$ distribution is extremely important when performing hypothesis testing in multiple linear regression.

Like the $\chi^2$ distribution, the $F$ distribution is skewed to the right.

```{r, echo = FALSE}
ggplot(data = data.frame(x = c(0, 20)), aes(x)) +
  stat_function(fun = dchisq, n = 101, args = list(df = 6)) + 
  stat_function(fun = df, n = 101, args = list(df1 = 6, df2 = 21), aes(colour="red")) + 
  ylab("") +
  scale_y_continuous(breaks = NULL) + 
  theme_bw() + 
  theme(legend.position = "none")
```

### Example

Suppose $X \sim F_{df1=6,df2=21}$.  

### a 
What is $Pr(X < 1.3)$?
```{r, include = TRUE}
# We want a probability (p)
# from an F distribution (f)
pf(q=1.3,df1=6,df2=21)
```

### b
Find $x$ such that $Pr(X > x) = 0.4$.
```{r, include = TRUE}
# We want a quantile (q)
# from an F distribution (f)
qf(1-0.4,df1=6,df2=21)
qf(0.4,df1=6,df2=21,lower.tail=FALSE)
```

The $F$ distribution is related to the $t$ distribution because if a random variable $T \sim t_{df=\nu}$, then $T^2 \sim F(df1=1,df2=\nu)$

## Notes for distribution calculations in R

- Starts with p: We are inputting a *quantile* to output a *probability*
- Starts with q: We are inputting a *probability* to output a *quantile*

- norm: We are using a *Normal* distribution with mean $\mu$ and standard deviation $\sigma$
- t: We are using a *t* distribution with degrees of freedom $df$
- f: We are using an *F* distribution with numerator degrees of freedom $df1$ and denominator degrees of freedom $df2$

# Statistical Inference

## Estimation

- **Estimation**: The category of statistical inference concerned with quantifying the specific value of a population parameter.

For example, if we have a random sample of data $x_1,x_2,\dots,x_n$ from a population, we can obtain an estimate of the population mean, $\mu$, by the sample mean $\bar{x}$.

Can we say that $\bar{x}$ equivalent to $\mu$?

**NO**, different samples produce different sample means.

We need to find a way to quantify the uncertainty of our estimate of the population mean (or other population parameter).

- **Confidence Interval**: A pair of values that provides a range of *plausible* values for the population parameter for a given level of confidence $C = 100 \times (1 - \alpha)$. $\alpha$ is the given level of significance

### Assumptions needed to calculate a confidence interval
- Data comes from a random sample from the population of interest.

Confidence intervals take the following general form: $$(\text{Parameter Estimate}) \pm (\text{Critical Value from }t\text{-distribution}) \times (\text{Estimate of Std. Error of Estimate}).$$

A $C\%$ confidence interval for a population mean, $\mu$ is written as $$\bar{x} \pm t_{n-1,1-\frac{\alpha}{2}} \times \frac{s_x}{\sqrt{n}},$$ where $t_{n-1,1-\frac{\alpha}{2}}$ is the $1-\frac{\alpha}{2}$ quantile of the $t$-distribution with $n-1$ degrees of freedom.

### Example
Recall the airfares dataset we previously uploaded into our R session.  Assume that the data comes from the population of interest, which in this case is all domestic flights out of O'Hare International Airport.  Calculate a 95\% confidence interval for the mean flight distance.

```{r}
xbar <- mean(dist)
n <- length(dist)
se <- sd(dist)/sqrt(n)
alpha <- 0.05
crit <- qt(p=1-alpha/2,df=n-1)
# c(-1,1) is what we input for plus/minus
xbar + c(-1,1)*crit*se
```

### Interpreting a confidence interval

We are $\boldsymbol{C\%}$ confident that the **true population parameter in the context of the given problem** is between **lower bound with units** and **upper bound with units**.

Go back to the previous example.  Interpret the 95\% CI in the context of the problem.

- We are 95% confident that the true average distance for all domestic flights out of O'Hare is between 513.80 and 1,119.26 miles.

## Hypothesis testing

- **Hypothesis Testing**: The category of statistical inference concerned with testing whether our estimated value for the population parameter is significantly different from the hypothesized value

### Procedure for performing a hypothesis test

1. Check that the assumptions needed to perform a hypothesis test are met.
  - Data comes from a random sample from the population of interest.
2. Specifically state the null hypothesis, $H_0$, and the alternative hypothesis, $H_a$.
  - Define $\mu_0$ as the hypothesized value of the population mean
  - $H_0: \mu = \mu_0$
  - $H_a: \mu > \mu_0$ (testing if our estimate is significantly greater)
  - $H_a: \mu < \mu_0$ (testing if our estimate is significantly less)
  - $H_a: \mu \neq \mu_0$ (testing if our estimate is significantly different)
3. Specify the level of significance, $\alpha$.
4. Calculate the test statistic.
5. Calculate the appropriate p-value for the hypothesis test.
6. Form a decision to either reject $H_0$ or fail to reject $H_0$.
7. State your conclusion.

### Example
In the airfares dataset, suppose it is believed that the average distance for domestic flights from O'Hare is 1000 miles.  Perform a hypothesis test for this belief with $\alpha = 0.05$.

$$H_0:\mu = 1000$$
$$H_a:\mu \neq 1000$$

```{r}
# x = data
# mu = mu under null hypothesis
# conf.level = 1 - alpha
t.test(x=dist,mu=1000,conf.level=0.95)
```
$t = -1.2848$
$p = 0.2172$

$p > \alpha$, so we fail to reject $H_0$.  We do not have statistically significant evidence to conclude that the average flight distance of all domestic flights from O'Hare is different than 1000 miles.

What if we wanted to see if the average flight distance is less than 1000 miles?

```{r}
t.test(x=dist,mu=1000,conf.level=0.95,alternative="less")
```

## Connection between confidence intervals and hypothesis testing.

**CI and HT connection**: If a confidence interval and hypothesis test are calculated on the \textbf{same observed dataset} where $H_a: \mu \neq \mu_0$ and the same $\alpha$ is used in both calculations, then $$\mu_0 \text{ is not inside the C% CI} \Leftrightarrow H_0 \text{ is rejected}$$ and $$\mu_0 \text{ is inside the C% CI} \Leftrightarrow H_0 \text{ is not rejected}.$$

### Example

Return to the confidence interval and hypothesis test we just conducted.  Are these answers compatible?

- Yes, we failed to reject $H_0: \mu = 1000$ at $\alpha = 0.05$ and 1000 is inside of our 95% confidence interval.

