---
title: "STAT 308 -- Chapter 3"
output: 
  pdf_document
header-includes:
  - \usepackage{graphicx}
  - \usepackage{float}
  - \usepackage{subfigure}
  - \usepackage{amsmath}
  - \usepackage{latexsym}
  - \usepackage{amssymb}
  - \usepackage{framed}
  - \usepackage{enumitem}
  - \usepackage{mathtools}
---

```{r setup, include = FALSE}
# This line of code tells the document all the display defaults

knitr::opts_chunk$set(echo = TRUE, cache = TRUE, include = TRUE, fig.align="center", fig.width = 3, fig.height = 3)
```

# Background Information

## Important Definitions

\begin{framed}
\textbf{Statistics:} the science and art of collecting, analyzing, and drawing conclusions from data.
\end{framed}

\begin{framed}
\textbf{Population of Interest:} Group of individuals we wish to know more information about
\end{framed}

\begin{framed}
\textbf{Sample:} Subset of the population of interest from which we can obtain information
\end{framed}

\begin{framed}
\textbf{Individuals:} the subjects/objects of the population of interest; can be people, but also business firms, common stocks, or any other object we want to study.
\end{framed}

\begin{framed}
\textbf{Variable:} any characteristic of an individual that we can measure and observe.
\end{framed}

## Uploading a dataset to R

```{r, include = TRUE}
# This is a very common R package that easily creates tidy data easier for
# analysis and make beautiful graphs!
# library(tidyverse)

# This tells R to find all files in the below folder
setwd("C:/Users/mstuart1/OneDrive - Loyola University Chicago/Classes/Fall 2022/STAT 308/Chapter 3")
# Slashes must be either \\ or / for R to read it

# This tells R to upload a text file as a data frame
airfares <- read.csv("../Data/airfares.csv")
head(airfares)

# This tells R to define an object as a variable from the data frame
# R code is data.frame$variable
dist <- airfares$Distance
```


## Parameters and Statistics

\begin{framed}
  \textbf{Population Parameter:} A numeric value that describes the characteristics of an entire population
\end{framed}
\begin{framed}
  \textbf{Sample Statistic:} A numeric value that describes the characteristics of the observed data from a sample
\end{framed}

Recall, we use \textbf{sample statistics} to make inference about \textbf{population parameters}.

Some important sample statistics:
\begin{framed}
\textbf{Sample Mean:} $\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i$ \\
\textbf{Sample Variance:} $s_x^2 = \frac{1}{n - 1}\sum_{i=1}^n (x_i - \bar{x})^2$ \\
\textbf{Sample Standard Deviation:} $s_x = \sqrt{s_x^2}$

\end{framed}

## Summary Statistics in R
```{r, include = TRUE}

mean(dist)
median(dist)
var(dist)
sd(dist)
summary(dist)

```

\vspace{6cm}

## Summary Graphs in R
```{r, include = TRUE}
# Create a histogram in R
hist(dist)

# Change the breaks (number of bars) in a histogram in R
hist(dist,breaks=7)

# Create a boxplot in R
boxplot(dist)
```

\newpage

# Random Variables and Distributions

\begin{framed}
\textbf{Random Variable:} denote a variable whose observed values may be considered outcomes of a stochastic or random experiment.  Random variables are typically denoted by a capital letter $X$, $Y$, etc., while observations are typically denoted by lowercase letters $x$, $y$, etc.
\end{framed}

Recall, a data frame contains \textbf{observations} from multiple \textbf{random variables} from a particular \textbf{sample} from the \textbf{population of interest}.

## Normal Distribution

If a random variable $X$ is normally distributed, this is denoted as $$X \sim \mathcal{N}(\mu_x,\sigma_x^2)$$ where $\mu_x$ is the mean of $X$ and $\sigma_x$ is the standard deviation of $X$.

### Example

Suppose $X \sim \mathcal{N}(2,4)$.  

### a 
What is $Pr(X > 3.5)$?

Recall that $Pr(X > 3.5) = 1 - Pr(X \leq 3.5)$
```{r}
# pnorm calculates a probability less than or equal x with a particular mean
# and standard deviation
1 - pnorm(3.5,mean=2,sd=2)

# If you set lower.tail = FALSE, this calculates probability greater than
pnorm(3.5,mean=2,sd=2,lower.tail=FALSE)
```

### b
What is the $0.35$ quantile/$35^{th}$ percentile of $X$?
```{r}
# qnorm a quantile for a given probability with a particular mean
# and standard deviation

qnorm(0.35,mean=2,sd=2)
```

## Central Limit Theorem

Define $\bar{X}$ as the random variable associated with the mean of a sample $\bar{x}$.

If a random variable $X$ is normally distributed with mean $\mu_x$ and standard deviation $\sigma_x$ OR the sample size $n_x$ is sufficiently large ($n_x > 30$), then the \textbf{sampling distribution of the sample mean}, $$\bar{X} \approx \mathcal{N}\left(\mu_x,\frac{\sigma_x^2}{n_x}\right)$$ or, in other words, $$\frac{\bar{X} - \mu_x}{\frac{\sigma_x}{\sqrt{n_x}}} \approx \mathcal{N}(0,1).$$  Recall $\frac{\sigma_x}{\sqrt{n_x}}$ is referred to as the standard error.  This is an important theorem used in estimation and inference, and will be used throughout the semester.

## Chi-squared $\chi^2$ Distribution

Let $S_x^2$ be the random variable associated with the sample variance $s_x^2$.  The chi-squared distribution can be used to describe the distribution of $S_x^2$, among other types of random variables.  More specifically, $$\frac{(n_x-1)S_x^2}{\sigma_x^2} \sim \chi^2_{df = n_x-1}.$$  The chi-squared distribution applies only to positive random variables and is significantly skewed to the right.

```{r, echo = FALSE}
ggplot(data = data.frame(x = c(0, 20)), aes(x)) +
  stat_function(fun = dchisq, n = 101, args = list(df = 6)) + 
  ylab("") +
  scale_y_continuous(breaks = NULL) + 
  theme_bw() + 
  theme(legend.position = "none")
```

### Example

Suppose $X \sim {\chi^2}_{df=10}$.  

### a 
What is $Pr(X < 5)$?
```{r}
pchisq(5,df=10)
```

### b
Find $x$ such that $Pr(X > x) = 0.6$.
```{r}
qchisq(0.6,df=10,lower.tail=FALSE)
```

## $t$ Distribution

Often times, the population standard deviation $\sigma_x$ is unknown in the purposes of the sampling distribution.  If this is the case, then we can substitute the sample standard deviation $s_x$ for the population standard deviation, $\sigma_x$.  And, in that case, $$\frac{\bar{X} - \mu_x}{\frac{s_x}{\sqrt{n_x}}} \sim t_{df = n_x-1}$$.

Like the normal distribution, the $t$ distribution is also symmetric and unimodal, but has fatter tails to account for the fact that we are using an estimate $s_x$ instead of $\sigma_x$.

```{r, echo = FALSE}
ggplot(data = data.frame(x = c(-4, 4)), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) +
  stat_function(fun = dt, n = 101, args = list(df = 3), aes(colour = "red")) +
  ylab("") +
  scale_y_continuous(breaks = NULL) + 
  theme_bw() + 
  theme(legend.position = "none")
```

### Example

Suppose $X \sim t_{df=10}$.  

### a 
What is $Pr(X < 2.2)$?
```{r}
pt(2.2,df=10)
```

### b
What is the $0.8$ quantile/$80^{th}$ percentile of $X$?
```{r}
qt(0.8,df=10)
```

## $F$ Distribution

Suppose now we have a new set of data from a random variable $Y$ with population mean $\mu_y$ and population variance $\sigma_y^2$.  Suppose the observed data has a sample mean $\bar{y}$ and sample variance $s_y^2$.  The $F$ distribution is an appropriate distribution for the ratio of the variances of the two random variables.  More specifically, $$\frac{S_x^2/\sigma_x^2}{S_y^2/\sigma_y^2} \sim F_{df1 = n_x - 1,df2 = n_y - 1}$$ where $df1$ is denoted as the \textbf{numerator degrees of freedom} and $df2$ is denoted as the \textbf{denominator degrees of freedom}.

Like the $\chi^2$ distribution, the $F$ distribution is skewed to the right.

```{r, echo = FALSE}
ggplot(data = data.frame(x = c(0, 20)), aes(x)) +
  stat_function(fun = dchisq, n = 101, args = list(df = 6)) + 
  stat_function(fun = df, n = 101, args = list(df1 = 6, df2 = 21), aes(colour="red")) + 
  ylab("") +
  scale_y_continuous(breaks = NULL) + 
  theme_bw() + 
  theme(legend.position = "none")
```

### Example

Suppose $X \sim F_{df1=6,df2=21}$.  

### a 
What is $Pr(X < 1.3)$?
```{r}
pf(1.3,df1=6,df2=21)
```
### b
Find $x$ such that $Pr(X > x) = 0.4$.
```{r}
qf(0.4,df1=6,df2=21,lower.tail=FALSE)
```

The $F$ distribution is related to the $t$ distribution because if a random variable $T \sim t_{df=\nu}$, then $T^2 \sim F_{df1=1,df2=\nu}$

## Notes for distribution calculations in R
p -- calculates probabilty for given quantile
q -- calculates quantile for a given probabilty
lower.tail == FALSE -- if using greater than probability

norm -- Normal distribution
chisq -- Chi-squared distribution
t -- t distribution
F -- F distribution

# Statistical Inference

## Estimation

\begin{framed}
  \textbf{Estimation:} The category of statistical inference concerned with quantifying the specific value of a population parameter.
\end{framed}

For example, if we have a random sample of data $x_1,x_2,\dots,x_n$ from a population, we can obtain an estimate of the population mean, $\mu$, by the sample mean $\bar{x}$.

Can we say that $\bar{x}$ equivalent to $\mu$?

\textbf{NO}, different samples produce different sample means.

We need to find a way to quantify the uncertainty of our estimate of the population mean (or other population parameter).

\begin{framed}
  \textbf{Confidence Interval:} A pair of values that provides a range of \textit{plausible} values for the population parameter for a given level of confidence $C = 100 \times (1 - \alpha)$.
\end{framed}

### Assumptions needed to calculate a confidence interval
\begin{itemize}
  \item Data comes from a random sample from the population of interest
\end{itemize}

Confidence intervals take the following general form: $$(\text{Parameter Estimate}) \pm (\text{Critical Value from }t\text{-distribution}) \times (\text{Estimate of Std. Error of Estimate}).$$

A $C\%$ confidence interval for a population mean, $\mu$ is written as $$\bar{x} \pm t_{n-1,1-\frac{\alpha}{2}} \times s_x,$$ where $t_{n-1,1-\frac{\alpha}{2}}$ is the $1-\frac{\alpha}{2}$ quantile of the $t$-distribution with $n-1$ degrees of freedom.

### Example
Recall the airfares dataset we previously uploaded into our \texttt{R} session.  Assume that the data comes from the population of interest, which in this case is all domestic flights out of O'Hare International Airport.  Calculate a 95\% confidence interval for the mean flight distance.

\vspace{6cm}

```{r}
```

### Interpreting a confidence interval

We are $\boldsymbol{C\%}$ confident that the \textbf{true population parameter in the context of the given problem} is between \textbf{lower bound with units } and \textbf{upper bound with units}.

Go back to the previous example.  Interpret the 95\% CI in the context of the problem.

\vspace{2cm}


## Hypothesis testing

\begin{framed}
  \textbf{Hypothesis Testing:} The category of statistical inference concerned with testing whether our estimated value for the population parameter is different enough from the hypothesized value
\end{framed}

### Procedure for performing a hypothesis test

\begin{enumerate}
  \item Check that the assumptions needed to perform a hypothesis test are met.
  \begin{itemize}
    \item Data comes from a random sample from the population of interest.
  \end{itemize}
  \item Specifically state the null hypothesis, $H_0$, and the alternative hypothesis, $H_a$.
  \item Specify the level of significance, $\alpha$.
  \item Calculate the test statistic.
  \item Calculate the appropriate p-value for the hypothesis test.
  \item Form a decision to either reject $H_0$ or fail to reject $H_0$.
  \item State your conclusion.
\end{enumerate}

### Example
In the airfares dataset, suppose it is believed that the average distance for domestic flights from O'Hare is 1000 miles.  Perform a hypothesis test for this belief with $\alpha = 0.05$.

```{r}
```

## Connection between confidence intervals and hypothesis testing.

\begin{framed}
  \textbf{CI and HT connection:} If a confidence interval and hypothesis test are calculated on the \textbf{same observed dataset} where $H_a: \mu \neq \mu_0$ and the same $\alpha$ is used in both calculations, then $$\mu_0 \text{ is not inside the C\% CI} \Leftrightarrow H_0 \text{ is rejected}$$ and $$\mu_0 \text{ is inside the C\% CI} \Leftrightarrow H_0 \text{ is not rejected}.$$
\end{framed}

### Example

Return to the confidence interval and hypothesis test we just conducted.  Are these answers compatible?


