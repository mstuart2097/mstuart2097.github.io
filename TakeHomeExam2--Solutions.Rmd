

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, include = TRUE,
fig.align="center",warnings = FALSE,fig.width = 5,fig.height=3)
```

This take home exam is worth a total of 100 points.  For the problems in which calculations are needed, please include your \texttt{R} code with your answers.  Turn in this exam to Sakai by Thursday, November 17 at 2:30 pm.

1.  [3 pts each] For each of the following problems with a sample size of $n$, calculate an appropriate test statistic and p-value for a linear model with $p-1$ total explanatory variables in the model. In other words, we have $p$ parameters in the linear model (one for each of the $p-1$ explanatory variables, and the intercept).

a. $n = 50$, $p = 4$, $SSM = 150.32$, $SSE = 230.44$.
```{r}
p <- 4
n <- 50
SSM <- 150.32
SSE <- 230.44
dfm <- p-1
dfe <- n-p
F.stat <- (SSM/dfm)/(SSE/dfe)
p.val <- pf(F.stat,dfm,dfe,lower.tail=FALSE)
F.stat
p.val
```

b. $n = 15$, $p = 3$, $SSM = 77.3$, $SSE = 125.44$.
```{r}
p <- 3
n <- 15
SSM <- 77.3
SSE <- 125.44
dfm <- p-1
dfe <- n-p
F.stat <- (SSM/dfm)/(SSE/dfe)
p.val <- pf(F.stat,dfm,dfe,lower.tail=FALSE)
F.stat
p.val
```

c. $n = 60$, $p = 6$, $SSM = 50.32$, $SSE = 250.78$.
```{r}
p <- 6
n <- 60
SSM <- 50.32
SSE <- 250.78
dfm <- p-1
dfe <- n-p
F.stat <- (SSM/dfm)/(SSE/dfe)
p.val <- pf(F.stat,dfm,dfe,lower.tail=FALSE)
F.stat
p.val
```

d. $n = 75$, $p = 5$, $SSM = 3.22$, $SSE = 9.87$.
```{r}
p <- 5
n <- 75
SSM <- 3.22
SSE <- 9.87
dfm <- p-1
dfe <- n-p
F.stat <- (SSM/dfm)/(SSE/dfe)
p.val <- pf(F.stat,dfm,dfe,lower.tail=FALSE)
F.stat
p.val
```

e. $n = 28$, $p = 2$, $SSM = 60.44$, $SSE = 514.32$.
```{r}
p <- 2
n <- 28
SSM <- 60.44
SSE <- 514.32
dfm <- p-1
dfe <- n-p
F.stat <- (SSM/dfm)/(SSE/dfe)
p.val <- pf(F.stat,dfm,dfe,lower.tail=FALSE)
F.stat
p.val
```

2. Use the below degrees of freedom and sums of squares from ANOVA tables from a multiple linear regression with three explanatory variables to answer the following questions.

- Model 1: $X_1$ as only explanatory variable

|     |df    |Sums of Squares  |
|-----|:-----|:----------------|
|Model |4 | 2341.53 |
|Error |95 | 1032.35 |

- Model 2: $X_1$ and $X_2$ as explanatory variables

|     |df    |Sums of Squares  |
|-----|:-----|:----------------|
|Model |5 | 2609.87 |
|Error |94 |764.01 |

- Model 3: $X_1$, $X_2$, and $X_3$ as explanatory variables

|     |df    |Sums of Squares  |
|-----|:-----|:----------------|
|Model |6 | 2701.22 |
|Error |93 |672.66 |

a. [2 pts] Is $X_1$ a categorical variable?  If it is, how many possible values are there for $X_1$?

- Yes, $p - 1 = 4$, so there are 5 possible values.

b. [2 pts] What is the sample size $n$ for the dataset?

- $n - 1 = 4 + 95 = 99$, so $n= 100$

c. [2 pts] What are the total sums of squares for the response variable $Y$?

- $SST = SSM + SSE = 2341.53 + 1032.35 = 3373.88$

d. [2 pts] What is the mean squared error for the model with $X_1$ and $X_2$ included.

- $MSE = 764.01/94 = 8.128$
 
e. [2 pts] What is the $r^2$ for the model with $X_1$, $X_2$, and $X_3$ included?

- $r^2 = 2701.22/3373.88 = 0.8006 = 80.06\%$

f. [2 pts] What is the additional explained variation in the response when we add $X_3$ to a model that already includes $X_1$ and $X_2$?

- $(2701.22 - 2609.87)/3373.88 = 0.02708 = 2.708\%$

g. [2 pts] What are the additional model sums of squares when we add $X_2$ and $X_3$ to a model that includes $X_1$?

- $2701.22 - 2341.53 = 359.69$

h. [2 pts] What are the model and error degrees of freedom for testing if including $X_1$, $X_2$ and $X_3$ in a model is statistically significant.

- $dfm = 6$, $df3 = 93$

i. [2 pts] Suppose we want to test if adding $X_3$ to a model that already includes $X_1$, $X_2$ is statistically significant.  What are the model and error degrees of freedom?

- $dfm = 6 - 5= 1$, $df3 = 93$

j. [4 pts] Find an F statistic and p-value for testing if adding $X_2$ to a model that already includes $X_1$ is statistically significant.

```{r}
SSM <- 1032.35 - 764.01
SSE <- 764.01
dfm <- 1
dfe <- 94
F.stat <- (SSM/dfm)/(SSE/dfe)
p.val <- pf(F.stat,dfm,dfe,lower.tail=FALSE)
F.stat
p.val
```

3.  For this problem, consider the mtcars dataset available in base R.  Use the command "?mtcars" to obtain information about the overall dataset, as well as the variables contained within it.  

a. Suppose we want to fit a linear model to predict a car's fuel efficiency by 

- Number of Cylinders

- Horsepower

- Weight

- Transmission

- Quarter Mile Time

i. [4 pts] Is there a potential issue with multicollinearity with this dataset?  If so, explain why and what variable should we consider removing.
```{r}
library(regclass)
mod <- lm(mpg ~ cyl + hp + wt + am + qsec,mtcars)
VIF(mod)
```

Number of cylinders, horsepower, and quarter mile time all have a VIF above 5, so there is an issue of multicollinearity.

ii. [4 pts] Perform a formal hypothesis test to determine if the explanatory variable with the highest VIF in the linear model in (a) is statistically significant if all of the other explanatory variable are included in the model.  Assume $\alpha = 0.05$.

- Reduced Model: $\hat{mpg} = \beta_0 + \beta_1 hp + \beta_2 wt + \beta_3 am + \beta_4 qsec$

- Full Model: $\hat{mpg} = \beta_0 + \beta_1 hp + \beta_2 wt + \beta_3 am + \beta_4 qsec + \beta_5 cyl$

$$H_0 : \beta_5 = 0$$
$$H_a : \beta_5 \neq 0$$

```{r}
mod_red <- lm(mpg ~ hp + wt + am + qsec,mtcars)
mod_full <- lm(mpg ~ hp + wt + am + qsec + cyl,mtcars)
anova(mod_red,mod_full)
```
$F = 0.0405$, $p = 0.8421$, so we fail to reject $H_0$ and conclude that adding number of cylinders to a model that already includes horsepower, weight, transmission, and quarter-mile time is not statistically significant in predicting fuel efficiency.

iii. [2 pts] Is there still an issue of multicollinearity after the variable with the highest VIF is removed?  Explain your answer.
```{r}
VIF(mod_red)
```
All the VIF values are now below 5, so there is not an issue of multicollinearity.

b. [2 pts] Check if the assumptions of homoskedasticity and normally distributed are violated for the original model in (a).  If they are not met, make a suggestion for how we can adjust our model.

```{r}
plot(mod_full,1)
plot(mod_full,2)
```
The two plots suggest that the assumptions of homoskedasticity and normality are not violated.

c. Suppose that, after discussion with fellow researchers, we decide to transform the response to log-fuel efficiency, and fit a linear model for log-fuel efficiency with all of the other variables still included.

- i. [3 pts] Write the least squares regression line.
```{r}
mod_full <- lm(log(mpg) ~ hp + wt + am + qsec + cyl,mtcars)
summary(mod_full)
```

$\hat{\log (mpg)} = 3.193 -0.0009 hp -0.1854 wt + 0.0747 am + 0.0283 qsec - 0.00787 cyl$

- ii. [3 pts] Interpret the parameter associated with number of cylinders in the context of the problem.

When the number of cylinders increases by 1, the predicted log fuel efficiency decreases by 0.00787 log mpg, holding all other variables constant.

- iii. [3 pts] Interpret the parameter associated with transmission in the context of the problem.

When the transmission of the car is manual, the predicted log fuel efficiency increases by 0.0747 log mpg over an automatic transmission, holding all other variables constant.

- iv. [4 pts] What do we predict the fuel efficiency to be for a car with 6 cylinders, 130 horsepower, weighs 2200 lbs, is a manual transmission, and has an 18 second quarter-mile time.
```{r}
new.data <- data.frame(cyl = 6, hp = 130, wt = 2.2, am = 1, qsec = 18)
exp(predict(mod_full,new.data))
```
24.79 mpg

- v. [3 pts] Interpret the $r^2$ for this model in the context of the problem.
```{r}
summary(mod_full)$r.squared
```
88.42% of the variation in log fuel efficiency can be predicted by our linear model with number of cylinders, horsepower, weight, transmission, and quarter-mile time included.

- vi. [4 pts] Perform a formal hypothesis test to see if our transformed linear model is statistically significant.  Assume $\alpha = 0.05$.

- Reduced Model: $\hat{\log(mpg)} = \beta_0$

- Full Model: $\hat{\log(mpg)} = \beta_0 + \beta_1 hp + \beta_2 wt + \beta_3 am + \beta_4 qsec + \beta_5 cyl$

$$H_0 : \beta_1 = \cdots = \beta_5 = 0$$
$$H_a: \text{At least one } \beta \neq 0$$
```{r}
mod_red <- lm(log(mpg) ~ 1,mtcars)
anova(mod_red,mod_full)
```
$F = 39.70$, $p = 2.303 \times 10^{-11}$, so we reject $H_0$ and conclude that a log-linear model predicting fuel efficiency by number of cylinders, horsepower, weight, transmission, and quarter-mile time included is statistically significant.

- vii. [2 pts] Determine if the assumptions for homoskedasticity and normality are violated for the transformed model.
```{r}
plot(mod_full,1)
plot(mod_full,2)
```
The plot suggest that the assumptions of homoskedasticity and normality are not violated.

4.  Consider the \texttt{lifeexp} dataset on the course webpage which contains information on countries from the year 2015.  The variables contained in the dataset are defined as follows:

- \texttt{Status}: Development status of the country

- \texttt{Life.Expectancy}: Country's average life expectancy in years

- \texttt{Adult.Mortality}: Number of deaths of people between 15 and 60 per 1000 population

- \texttt{infant.deaths}: Number of infant deaths per 1000 population

- \texttt{Measles}: Number of measles cases per 1000 population

- \texttt{BMI}: average BMI for entire population

- \texttt{under.five.deaths}: Number of Under five deaths per 1000 population

- \texttt{Polio}: Polio immunization coverage for 1 year olds

- \texttt{Diptheria}: Diphtheria tetanus toxoid and pertussis immunization coverage for 1 year olds

- \texttt{HIV.AIDS}: Deaths per 1000 live births from HIV/AIDS (0-4 year olds)

- \texttt{Income.composition.of.resources}: Human Development Index in terms of income composition of resources (from 0 to 1)

- \texttt{Schooling}: Number of Years of Schooling

We are interested in creating a regression model that predicts a country's life expectancy based on the other factors in the dataset.

a.  Suppose we know that both adult mortality and income composition of resources are significant in a linear model that predicts life expectancy.  We would like to know if adding development status signficantly improves the predictive ability of our model.

- i. [3 pts] Report the least squares regression lines for predicting life expectancy for both developed and developing countries with adult mortality and income composition of resources included.
```{r}
lifeexp <- read.csv("../data/lifeexp.csv")
mod_full <- lm(Life.expectancy ~ Status + Adult.Mortality + Income.composition.of.resources,lifeexp)
summary(mod_full)
```
$\hat{lifeexp}_{developed} = 50.33 - 0.02383Mort + 36.63ICOR$

$\hat{lifeexp}_{developing} = 50.33 - 0.4298 - 0.02383Mort + 36.63ICOR$

- ii. [3 pts] What is the expected difference in life expectancy between developed and developing countries for fixed levels of adult mortality and income composition of resources?

-0.4298 years

- iii. [4 pts] Perform a formal hypothesis test to determine if adding development status to our linear model significantly improves our model's predictive ability for life expectancy.  Be sure to include all pieces of information needed to perform a hypothesis test.  Assume $\alpha = 0.05$.

- Reduced Model: $\hat{lifeexp} = \beta_0 + \beta_1 Mort + \beta_2 ICOR$

- Full Model: $\hat{lifeexp} = \beta_0 + \beta_1 Mort + \beta_2 ICOR + \beta_3Development$

$$H_0: \beta_3 = 0$$
$$H_0: \beta_3 \neq 0$$

```{r}
mod_red <- lm(Life.expectancy ~ Adult.Mortality + Income.composition.of.resources,lifeexp)
anova(mod_red,mod_full)
```
$F = 0.3655$, $p = 0.5463$, we fail to reject $H_0$ and say that adding development status to a model that already includes mortality rate and income composition of resources is not statistically significant in predicting life expectancy.

- iv. [5 pts] Perform a formal hypothesis test to determine if EITHER the linear relationship between adult mortality and life expectancy OR the linear relationship between income composition of resources and life expectancy is different for the two different development statuses.  Be sure to include all pieces of information needed to perform a hypothesis test.  Assume $\alpha = 0.05$.

- Reduced Model: $\hat{lifeexp} = \beta_0 + \beta_1 Mort + \beta_2 ICOR + \beta_3Development$

- Full Model: $\hat{lifeexp} = \beta_0 + \beta_1 Mort + \beta_2 ICOR + \beta_3Development + \beta_4 Mort \times Development + \beta_5 ICOR \times Development$

$$H_0: \beta_4 = \beta_5 = 0$$
$$H_a: \text{At least one } \beta \neq 0$$
```{r}
mod_red <- lm(Life.expectancy ~ Status + Adult.Mortality + Income.composition.of.resources,lifeexp)
mod_full <- lm(Life.expectancy ~ Status + Adult.Mortality + Income.composition.of.resources + Status:Adult.Mortality +
Status:Income.composition.of.resources,lifeexp)
anova(mod_red,mod_full)
```
$F = 1.190$, $p = 0.3069$.  We fail to reject $H_0$ and conclude that the linear relationship between income composition of resources and life expectancy and the linear relationship between adult mortality and life expectancy is not significantly different between the different development statuses.


b.  We now would like to determine the "best" overall prediction model for life expectancy using all of the other variables in our dataset as predictors.

- i. [3 pts] Should we include any polynomial terms in our model? Explain your reasoning.
```{r}
```

- Very subjective, willing to listen to any answers.

- ii. [6 pts]  Assume we include no polynomial terms or interaction terms.  Use backward selection with AIC as your decision criteria to eliminate variables from your model.  Be sure to state which variables were eliminated at each step.
```{r}
mod_max <- lm(Life.expectancy ~ .,lifeexp)
mod_best <- step(mod_max)
```

Schooling, measles, BMI, polio, development status.

- iii. [3 pts] Write your final regression model for predicting life expectancy.
```{r}
summary(mod_best)
```
$\hat{LifeExp} = 48.69 - 0.01783 Mort + 0.04473 InfDeaths - 0.0368 UndDeaths - 0.6387 HIVAIDS + 34.22ICOR$

- iv. [2 pts] Report the $r^2$ and mean squared error for your final linear model.
```{r}
summary(mod_best)$r.squared
summary(mod_best)$s^2
```

