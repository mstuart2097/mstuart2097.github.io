```{r setup, include = TRUE, echo=FALSE}
# This line of code tells the document all the display defaults

knitr::opts_chunk$set(echo = TRUE, include = TRUE, fig.align="center",warnings = FALSE,fig.width = 5,fig.height=3)
```

# Background Information

We have introduced the concept of multiple linear regression where we introduce linearly regressing a continuous response variable $Y$ on multiple explanatory variables $X_1$, $X_2$, $\dots$, $X_{p-1}$.  This chapter will introduce how we perform hypothesis tests on these regression lines.

## Test for Overall Significance of a multiple linear regression model

Recall, the method for testing for a significant linear relationship in simple linear regression.

Reduced Model: $\hat{Y} = \beta_0$

Full Model: $\hat{Y} = \beta_0 + \beta_1$

The null and alternative hypotheses are:  $$H_0:\beta_1 = 0$$ $$H_a:\beta_1 \neq 0$$

In other words, if we want to test whether or not AllGreen's net sales has a statistically significant linear relationship with its advertising spending, we are testing to see if the solid line in the below graph is significantly better at predicting $Y$, AllGreen's net sales, than the horizontal dashed line.

```{r}
allgreen <- read.csv("../data/AllGreen.csv")
plot(NetSales ~ Advertising,allgreen)
abline(lm(NetSales ~ Advertising,allgreen))
abline(h=mean(allgreen$NetSales),lty=2)
```



What is this equivalent in multiple dimensions?

```{r, echo=FALSE}
# You do NOT need to run this code.  This is used only for in class visualization.
## Estimation of the regression plane
mod <- lm(NetSales ~ Advertising + SqFt,allgreen)
cf.mod <- coef(mod)

### Calculate z on a grid of x-y values
x1.seq <- seq(min(allgreen$Advertising),max(allgreen$Advertising),length.out=25)
x2.seq <- seq(min(allgreen$SqFt),max(allgreen$SqFt),length.out=25)
z.mod <- t(outer(x1.seq, x2.seq, function(x,y) cf.mod[1]+cf.mod[2]*x+cf.mod[3]*y))
z.same <- matrix(mean(allgreen$NetSales),nrow=25,ncol=25)

#### Draw the plane with "plot_ly" and add points with "add_trace"
library(plotly)
plot_ly(allgreen,
        x=~Advertising, # First explanatory variable
        y=~SqFt, # Second explanatory variable,
        z=~NetSales) %>% # Response variable
  add_markers() %>%
  add_trace(x=x1.seq, y=x2.seq, z=z.mod, type="surface",
           colorscale = list(c(0, 1), c("blue", "blue"))) %>%
  add_trace(x=x1.seq, y=x2.seq, z=z.same, type="surface",
           colorscale = list(c(0, 1), c("grey", "grey"))) %>%
  layout(scene = list(
    aspectmode = "manual", aspectratio = list(x=1, y=1, z=1)))
```

What are the null and alternative hypotheses for testing if the non-horizontal linear plane is better at predicting than the horizontal linear plane?

- Again, think of this question in terms of a reduced and a full model.

Reduced Model:

Full Model:

$$H_0:$$

$$H_a:$$

In general for a total of $p-1$ explanatory variables:

$$H_0:$$

$$H_a:$$

## Test Statistic and p-value

Recall the ANOVA table:

|     |df    |Sums of Squares  |Mean Square  |F Value  |Pr(>f)   |
|-----|:-----|:----------------|:------------|:--------|:--------|
|Model|$p-1$ | $\text{SSM}$             | $\text{MSM} = \frac{\text{SSM}}{p-1}$| $F=\frac{\text{MSM}}{\text{MSE}}$ | $P(F_{df1=p-1,df2=n-p} > F)$ |
|Error|$n-p$ | $\text{SSE}$              | $\text{MSE} = \frac{\text{SSE}}{n-p}$|         |         |
|Total|$n-1$ | $\text{SST} = \text{SSM} + \text{SSE}$ |             |         |         |   

This provides us test statistics and p-values for the test for overall significance of our linear regression model.

- F-statistic: $\frac{\text{Explained Variance}}{\text{Unexplained Variance}} = \frac{\text{SSM}/(p-1)}{\text{SSE}/(n-p)}$

  - $p-1$ are the degrees of freedom for the linear model
  
  - $n-p$ are the error degrees of freedom

- $p-value$: Calculated using an F-distribution with $p-1$ and $n-p$ degrees of freedom

### Example

Perform a test for a significant linear regression for All Green's net sales with both advertising dollars and square footage as predictors. 

```{r}
```

# Test for Partial Significance of a multiple linear regression model

Now, suppose we know that advertising is a significant linear predictor for net sales for All Green.  Does adding square footage to our linear model significantly improve the prediction of net sales?

```{r, echo=FALSE}
# # You do NOT need to run this code.  This is used only for in class visualization.
# ## Estimation of the regression plane
mod <- lm(NetSales ~ Advertising + SqFt,allgreen)
cf.mod <- coef(mod)
mod2 <- lm(NetSales ~ Advertising,allgreen)
cf.mod2 <- coef(mod2)
#
### Calculate z on a grid of x-y values
x1.seq <- seq(min(allgreen$Advertising),max(allgreen$Advertising),length.out=25)
x2.seq <- seq(min(allgreen$SqFt),max(allgreen$SqFt),length.out=25)
z.mod <- t(outer(x1.seq, x2.seq, function(x,y) cf.mod[1]+cf.mod[2]*x+cf.mod[3]*y))
z.mod2 <- t(outer(x1.seq, x2.seq, function(x,y) cf.mod2[1]+cf.mod[2]*x))

#### Draw the plane with "plot_ly" and add points with "add_trace"
library(plotly)
plot_ly(allgreen,
        x=~Advertising, # First explanatory variable
        y=~SqFt, # Second explanatory variable,
        z=~NetSales) %>% # Response variable
  add_markers() %>%
  add_trace(x=x1.seq, y=x2.seq, z=z.mod, type="surface",
            colorscale = list(c(0, 1), c("blue", "blue"))) %>%
  add_trace(x=x1.seq, y=x2.seq, z=z.mod2, type="surface",
            colorscale = list(c(0, 1), c("grey", "grey"))) %>%
  layout(scene = list(
    aspectmode = "manual", aspectratio = list(x=1, y=1, z=1)))
```

Reduced Model:

Full Model:

$$H_0:$$

$$H_a:$$
In general, for a total of $p-1$ explanatory variables and we assume the first $k-1$ variables are already included in the model:

$$H_0:$$

$$H_a:$$
How do we find a test statistic and p-value?

We know that, for any linear model: $\text{SST} = \text{SSM} + \text{SSE}$

- For the reduced model: $\text{SST} = \text{SSM}_{reduced} + \text{SSE}_{reduced}$

- For the full model: $\text{SST} = \text{SSM}_{full} + \text{SSE}_{full}$

What happens to the sums of squares when we add explanatory variables from the reduced model to get the full model?

- More formally, we want to test if the additional variance that is explained by the full model is statistically significant

- F-statistic: $\frac{\text{New Explained Variance}}{\text{Still Unexplained Variance}} = \frac{(\text{SSE}_{reduced} - \text{SSE}_{full})/(p-k)}{\text{SSE}_{full}/(n-p)}$

  - $p-k$ are the **additional** degrees of freedom to the linear model
  
  - $n-p$ are the error degrees of freedom

- $p-value$: Calculated using an F-distribution with $p-k$ and $n-p$ degrees of freedom

### Example

Formally answer the question: Does adding square footage to our linear model that already includes advertising spending significantly improve the prediction of net sales?

```{r}
```

### Example

Suppose now I want to test whether or not including both square footage and amount of inventory significantly improves the linear model with advertising already included.  How would I perform this?

```{r}
```

### Question

What is the additional percent variation in net sales of the AllGreen stores that can be explained by adding square footage to our linear model that already includes advertising spending?



What is the additional percent variation in net sales of the AllGreen stores that can be explained by adding square footage and amount of inventory to our linear model that already includes advertising spending?

