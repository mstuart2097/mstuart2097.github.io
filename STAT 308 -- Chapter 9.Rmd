---
title: "STAT 308 -- Chapter 9"
output: 
  pdf_document
header-includes:
  - \usepackage{framed}
---

```{r setup, include = TRUE, echo=FALSE}
# This line of code tells the document all the display defaults

knitr::opts_chunk$set(echo = TRUE, include = TRUE, fig.align="center",warnings = FALSE,fig.width = 5,fig.height=3)
```

# Background Information

We have introduced the concept of multiple linear regression where we introduce linearly regressing a continuous response variable $Y$ on multiple explanatory variables $X_1$, $X_2$, $\dots$, $X_p$.  This chapter will introduce how we perform hypothesis tests on these regression lines.

# Test for Overall Regression

Recall, the method for testing for a significant linear relationship in simple linear regression.

The null and alternative hypotheses are:  $$H_0:\beta_1 = 0$$ $$H_a:\beta_1 \neq 0$$. 


In other words we are testing to see if the solid line in the below graph is significantly better at predicting $Y$ than the horizontal dashed line.
```{r}
allgreen <- read.csv("../data/AllGreen.csv")
plot(NetSales ~ SqFt,allgreen)
abline(lm(NetSales ~ SqFt,allgreen))
abline(h=mean(allgreen$NetSales),lty=2)
```



What is this equivalent in multiple dimensions?

```{r, echo=FALSE}
## You do NOT need to run this code.  This is used only for in class visualization.
### Estimation of the regression plane
# mod <- lm(NetSales ~ Advertising + SqFt,allgreen)
# cf.mod <- coef(mod)
# 
# ### Calculate z on a grid of x-y values
# x1.seq <- seq(min(allgreen$Advertising),max(allgreen$Advertising),length.out=25)
# x2.seq <- seq(min(allgreen$SqFt),max(allgreen$SqFt),length.out=25)
# z.mod <- t(outer(x1.seq, x2.seq, function(x,y) cf.mod[1]+cf.mod[2]*x+cf.mod[3]*y))
# z.same <- matrix(mean(allgreen$NetSales),nrow=25,ncol=25)
# 
# #### Draw the plane with "plot_ly" and add points with "add_trace"
# library(plotly)
# plot_ly(allgreen,
#         x=~Advertising, # First explanatory variable
#         y=~SqFt, # Second explanatory variable,
#         z=~NetSales) %>% # Response variable
#   add_markers() %>%
#   add_trace(x=x1.seq, y=x2.seq, z=z.mod, type="surface",
#            colorscale = list(c(0, 1), c("blue", "blue"))) %>%
#   add_trace(x=x1.seq, y=x2.seq, z=z.same, type="surface",
#            colorscale = list(c(0, 1), c("grey", "grey"))) %>%
#   layout(scene = list(
#     aspectmode = "manual", aspectratio = list(x=1, y=1, z=1)))
```

What is the null and alternative hypothesis equivalent of testing if the non-horizontal linear plane is better at predicting than the horizontal linear plane?

## Test Statistic and p-value

Recall the ANOVA table from simple linear regression:

\begin{tabular}{l|l|l|l|l|l}
& df & Sums of Squares & Mean Square & f Value & Pr(>f) \\
\hline
Model & 1 & $\text{SSM} = \text{SSY} - \text{SSE}$ & $\text{MSM} = \frac{\text{SSM}}{1}$ & $\frac{\text{MSM}}{\text{MSE}}$ & $Pr(F_{1,n-2} > \frac{\text{MSM}}{\text{MSE}})$ \\
Error & $n-2$ & $\text{SSE} = \sum_{i = 1}^n (y_i - \hat{y}_i)^2$ & $\text{MSE} = \frac{\text{SSE}}{n - 2}$ & & \\
\hline
Total & $n-1$ & $\text{SSY} = \sum_{i = 1}^n (y_i - \bar{y})^2$ & & & 
\end{tabular}

We said that the appropriate test statistic is $\frac{MSM}{MSE}$ and p-value is $Pr(F_{1,n-2} > \frac{\text{MSM}}{\text{MSE}})$.  This is because, under the null the null hypothesis, the sum of squares for the model and the sum of squared errors are both independent estimates of the same variance, or equivalently, the linear model with $X$ does not explain a significant amount of the variation in $Y$.

How does this compare with the full ANOVA table for multiple linear regression?

\begin{tabular}{l|l|l|l|l|l}
& df & Sums of Squares & Mean Square & f Value & Pr(>f) \\
\hline
Model & $p$ & $\text{SSM} = \text{SSY} - \text{SSE}$ & $\text{MSM} = \frac{\text{SSM}}{p}$ & $\frac{\text{MSM}}{\text{MSE}}$ & $Pr(F_{p,n-(p+1)} > \frac{\text{MSM}}{\text{MSE}})$ \\
Error & $n-(p+1)$ & $\text{SSE}$ & $\text{MSE} = \frac{\text{SSE}}{n - (p+1)}$ & & \\
\hline
Total & $n-1$ & $\text{SSY} = \sum_{i = 1}^n (y_i - \bar{y})^2$ & & & 
\end{tabular}

This provides us test statistics and p-values for the test for overall significance of our linear regression model.

### Example

Perform a test for a significant linear regression for All Green's net sales with both advertising dollars and square footage as predictors. 

```{r}
```

# Test for Partial Significance

Now, suppose we know that advertising is a significant linear predictor for net sales for All Green.  Does adding square footage to our linear model significantly improve the prediction of net sales?

```{r, echo=FALSE}
# # You do NOT need to run this code.  This is used only for in class visualization.
# ## Estimation of the regression plane
mod <- lm(NetSales ~ Advertising + SqFt,allgreen)
cf.mod <- coef(mod)
mod2 <- lm(NetSales ~ Advertising,allgreen)
cf.mod2 <- coef(mod2)
# 
# ### Calculate z on a grid of x-y values
# x1.seq <- seq(min(allgreen$Advertising),max(allgreen$Advertising),length.out=25)
# x2.seq <- seq(min(allgreen$SqFt),max(allgreen$SqFt),length.out=25)
# z.mod <- t(outer(x1.seq, x2.seq, function(x,y) cf.mod[1]+cf.mod[2]*x+cf.mod[3]*y))
# z.mod2 <- t(outer(x1.seq, x2.seq, function(x,y) cf.mod2[1]+cf.mod[2]*x))
# 
# #### Draw the plane with "plot_ly" and add points with "add_trace"
# library(plotly)
# plot_ly(allgreen,
#         x=~Advertising, # First explanatory variable
#         y=~SqFt, # Second explanatory variable,
#         z=~NetSales) %>% # Response variable
#   add_markers() %>%
#   add_trace(x=x1.seq, y=x2.seq, z=z.mod, type="surface",
#             colorscale = list(c(0, 1), c("blue", "blue"))) %>%
#   add_trace(x=x1.seq, y=x2.seq, z=z.mod2, type="surface",
#             colorscale = list(c(0, 1), c("grey", "grey"))) %>%
#   layout(scene = list(
#     aspectmode = "manual", aspectratio = list(x=1, y=1, z=1)))
```

If our regression line is $\hat{NetSales} = \beta_0 + \beta_1 Advertising + \beta_2 Sales$, then the null and alternative hypotheses are:


\vspace{2cm}

What about the test statistic and p-value?

## Using summary(mod)

One way is to note that, generically speaking:  $$t = \frac{\hat{\beta}_k - \beta_k}{s_{\hat{\beta}_k}} \sim t_{df = n-(p+1)}$$ for $k = 0,1,\dots,p$.  This naturally leads to a test statistic: $$t = \frac{\hat{\beta}_k - 0}{s_{\hat{\beta}_k}} = \frac{\hat{\beta}_k}{s_{\hat{\beta}_k}}$$.  We can find this information using \texttt{summary(mod)}.

```{r}

```

## Using anova(mod)

The ANOVA table can actually be broken down into different sums of squares for each variable added to the model!

\begin{tabular}{l|l|l|l|l|l}
& df & Sums of Squares & Mean Square & f Value & Pr(>f) \\
\hline
$X_1$ & $1$ & $\text{SS}(X_1)$ & $\text{MS}(X_1)$ & $\frac{\text{MS}(X_1)}{\text{MSE}}$ & $Pr(F_{1,n-(p+1)} > f)$ \\
$X_2|X_1$ & $1$ & $\text{SS}(X_2|X_1)$ & $\text{MS}(X_2|X_1)$ & $\frac{\text{MS}(X_2|X_1)}{\text{MSE}}$ & $Pr(F_{1,n-(p+1)} > f)$ \\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
$X_p|X_1,\dots,X_{p-1}$& $1$ & $\text{SS}(X_p|X_1,\dots,X_{p-1})$ & $\text{MS}(X_p|X_1,\dots,X_{p-1})$ & $\frac{\text{MS}(X_p|X_1,\dots,X_{p-1})}{\text{MSE}}$ & $Pr(F_{1,n-(p+1)} > f)$ \\
Error & $n-(p+1)$ & $\text{SSE}$ & $\text{MSE} = \frac{\text{SSE}}{n - (p+1)}$ & & \\
\hline
Total & $n-1$ & $\text{SSY} = \sum_{i = 1}^n (y_i - \bar{y})^2$ & & & 
\end{tabular}

where 

- $\text{SS}(X_1)$ are the sums of squares for the model with only $X_1$ as a predictor (i.e. $\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_{1i}$) 
- $\text{SS}(X_2 | X_1) = \text{SS}(X_1,X_2) - \text{SS}(X_1)$ (i.e. the additional model sums of squares when $X_2$ is added as a predictor to the model with $X_1$ already included)
- $\text{SS}(X_p | X_1,X_2,\dots,X_{p-1}) = \text{SS}(X_1,X_2,\dots,X_{p}) - \text{SS}(X_1,X_2,\dots,X_{p-1})$ (i.e. the additional model sums of squares when $X_p$ is added as a predictor to the model with $X_1,X_2,\dots,X_{p-1}$ already included)
- $\text{SSE}$ are the sum of squared errors for the model with all $p$ predictor variables included.

Let's see the differences between the simple linear regression ANOVA table and the multiple linear regression ANOVA table.

```{r}
```

Why are the results same for answering the does adding square footage to our linear model with advertising already included significantly improve the prediction of net sales for the \texttt{summary(mod)} method and the \texttt{anova(mod)} method?  Recall that if a test statistic, $t$, follows a t-distribution with $n-(p+1)$ degrees of freedom, then $$t^2 \sim F_{df1=1,df2=n-(p+1)}$$

### Example

Suppose now I want to test whether or not including both square footage and amount of inventory significantly improves the linear model with advertising already included.  How would I perform this?

\begin{framed}
\textbf{Reduced Model}: Model with some (but not all) explanatory variables excluded $$Y \sim \mathcal{N}(\beta_0 + \beta_1X_1 + \dots + \beta_kX_k,\sigma^2)$$
\textbf{Full Model}: Model with all explanatory variables included.
$$Y \sim \mathcal{N}(\beta_0 + \beta_1X_1 + \dots + \beta_pX_p,\sigma^2)$$ where $p > k$
\end{framed}

Just like we said $F = \frac{\text{SS}(X_p|X_1,\dots,X_{p-1})/1}{SSE/(n-(p+1))}$ can be used as a test statistic, we can obtain a similar test statistic for testing if adding multiple explanatory variables can improve our model's predictability:

$$F = \frac{\text{SS}(X_{k+1},\dots,X_p | X_1,X_2,\dots,X_k)/(p-k)}{SSE/(n-(p+1))}$$

where $\text{SS}(X_{k+1},\dots,X_p | X_1,X_2,\dots,X_k)$ are the model sums of squares added when going from the reduced model ($k$ predictors) to the full model ($p$ predictors).  This test statistic under $H_0$ follows an $F$-distribution with $p-k$ and $n-(p+1)$ degrees of freedom.  Another way to write this statistic is $$F = \frac{(\text{SSE}_{reduced} - \text{SSE}_{full})/(df_{reduced} - df_{full})}{\text{SSE}_{full}/df_{full}}$$ where 

- $\text{SSE}_{reduced}$ and $df_{reduced}$ are the sum of squared errors and error degrees of freedom for the reduced model
- $\text{SSE}_{full}$ and $df_{full}$ are the sum of squared errors and error degrees of freedom for the reduced model

Not only can ANOVA break down the sums of squares by each variable for a given model, it can also compare the sums of squares by two competing models!

\begin{table}[h!]
\centering
\begin{tabular}{l|l|l|l|l|l}
Res.df & SSE & df & Sum of Squares Added & F & p-value \\
\hline
$df_{reduced}$ & $\text{SSE}_{reduced}$ &&&&\\
$df_{full}$ & $\text{SSE}_{full}$ & $df_{reduced} - df_{full}$ & $\text{SSE}_{reduced} - \text{SSE}_{full}$ & $F$ & p-value \\
\end{tabular}
\end{table}

